{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14f89cb-bcca-4359-9268-1b058be54da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_metadata = True # will create a list of all csv files in the s3 bucket\n",
    "cache_http_calls = True # TTL for 1 hour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590969e0-3649-48e6-b6eb-5325a8423c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85719e4a-ce5a-457b-997c-a5a15565ec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0f9963ff-5e5c-4bbc-a922-e17bf04848be;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      ":: resolution report :: resolve 137ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0f9963ff-5e5c-4bbc-a922-e17bf04848be\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "21/10/03 14:59:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version = 3.1.2\n",
      "pyspark version = 3.1.2\n",
      "Hadoop version = 3.2.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import cachetools\n",
    "\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:3.2.0\" pyspark-shell'\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# !pip install boto3\n",
    "# !pip install cachetools\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.threads.max\", 10)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "sc = pyspark.SparkContext(\"local[*]\", conf = sparkConf, appName = \"s_p_challenge\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "print(f\"spark version = {spark.version}\")\n",
    "print(f\"pyspark version = {pyspark.__version__}\")\n",
    "print(f\"Hadoop version = {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05feea9a-a15c-416f-b0ba-9db0619b4d86",
   "metadata": {},
   "source": [
    "### Download data and metadata from gdelt-open-data S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5bcd620-28c4-406d-8251-c0d494eb76e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tableId</th>\n",
       "      <th>dataType</th>\n",
       "      <th>Empty</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GLOBALEVENTID</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>NULLABLE</td>\n",
       "      <td>Globally unique identifier assigned to each ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SQLDATE</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>NULLABLE</td>\n",
       "      <td>Date the event took place in YYYYMMDD format. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tableId dataType     Empty  \\\n",
       "0  GLOBALEVENTID  INTEGER  NULLABLE   \n",
       "1        SQLDATE  INTEGER  NULLABLE   \n",
       "\n",
       "                                         Description  \n",
       "0  Globally unique identifier assigned to each ev...  \n",
       "1  Date the event took place in YYYYMMDD format. ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2 event headers: https://github.com/linwoodc3/gdelt2HeaderRows/blob/master/schema_csvs/GDELT_2.0_Events_Column_Labels_Header_Row_Sep2016.csv\n",
    "headers = pd.read_csv('headers.csv')\n",
    "headers.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49635d23-8e40-4a04-a42e-3f1cb411190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_metadata():\n",
    "    '''\n",
    "    Download metadata from https://s3.console.aws.amazon.com/s3/buckets/gdelt-open-data?region=us-east-1\n",
    "    '''\n",
    "    s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED), region_name='us-east-1')\n",
    "    s3_events = s3.list_objects_v2(Bucket='gdelt-open-data', Prefix='v2/events/')\n",
    "    s3_all_events = []\n",
    "\n",
    "    is_truncated = True\n",
    "    continuation_token = None\n",
    "\n",
    "    while is_truncated: \n",
    "        if continuation_token:\n",
    "            s3_events = s3_events = s3.list_objects_v2(Bucket='gdelt-open-data', Prefix='v2/events/', ContinuationToken=continuation_token)\n",
    "        else: \n",
    "            s3_events = s3_events = s3.list_objects_v2(Bucket='gdelt-open-data', Prefix='v2/events/')\n",
    "        s3_all_events.append(s3_events)\n",
    "        is_truncated = s3_events['IsTruncated']\n",
    "        if 'NextContinuationToken' in s3_events: \n",
    "            continuation_token = s3_events['NextContinuationToken']\n",
    "    print('Total number of iterations to the S3 list objects = {:,}'.format(len(s3_all_events)))\n",
    "    s3_actual_events = []\n",
    "    for s3_events in s3_all_events:\n",
    "        s3_actual_events.extend(s3_events['Contents'])\n",
    "    print('Total number of files in the S3 bucket = {:,}'.format(len(s3_actual_events)))\n",
    "    return s3_actual_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5279ece-4a5e-487c-8030-bef643efd4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of iterations to the S3 list objects = 144\n",
      "Total number of files in the S3 bucket = 143,462\n"
     ]
    }
   ],
   "source": [
    "if download_metadata:\n",
    "    events_metadata = pd.DataFrame(download_metadata())\n",
    "    events_metadata.sort_values(by='LastModified', inplace=True, ascending=False)\n",
    "    events_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20bd004-3f93-49e1-a310-48266e3a8b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 15:01:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events in current file: 1772\n"
     ]
    }
   ],
   "source": [
    "# Example uses GDELT dataset found here: https://aws.amazon.com/public-datasets/gdelt/\n",
    "events = spark.read.csv(\"s3a://gdelt-open-data/v2/events/20190416151500.export.csv\", header=False, sep='\\t', inferSchema=True)\n",
    "print(f\"Total number of events in current file: {events.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbdd466a-d6a2-4650-9171-d4a0e464f6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GLOBALEVENTID: integer (nullable = true)\n",
      " |-- SQLDATE: date (nullable = true)\n",
      " |-- MonthYear: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- FractionDate: double (nullable = true)\n",
      " |-- Actor1Code: string (nullable = true)\n",
      " |-- Actor1Name: string (nullable = true)\n",
      " |-- Actor1CountryCode: string (nullable = true)\n",
      " |-- Actor1KnownGroupCode: string (nullable = true)\n",
      " |-- Actor1EthnicCode: string (nullable = true)\n",
      " |-- Actor1Religion1Code: string (nullable = true)\n",
      " |-- Actor1Religion2Code: string (nullable = true)\n",
      " |-- Actor1Type1Code: string (nullable = true)\n",
      " |-- Actor1Type2Code: string (nullable = true)\n",
      " |-- Actor1Type3Code: string (nullable = true)\n",
      " |-- Actor2Code: string (nullable = true)\n",
      " |-- Actor2Name: string (nullable = true)\n",
      " |-- Actor2CountryCode: string (nullable = true)\n",
      " |-- Actor2KnownGroupCode: string (nullable = true)\n",
      " |-- Actor2EthnicCode: string (nullable = true)\n",
      " |-- Actor2Religion1Code: string (nullable = true)\n",
      " |-- Actor2Religion2Code: string (nullable = true)\n",
      " |-- Actor2Type1Code: string (nullable = true)\n",
      " |-- Actor2Type2Code: string (nullable = true)\n",
      " |-- Actor2Type3Code: string (nullable = true)\n",
      " |-- IsRootEvent: integer (nullable = true)\n",
      " |-- EventCode: integer (nullable = true)\n",
      " |-- EventBaseCode: integer (nullable = true)\n",
      " |-- EventRootCode: integer (nullable = true)\n",
      " |-- QuadClass: integer (nullable = true)\n",
      " |-- GoldsteinScale: double (nullable = true)\n",
      " |-- NumMentions: integer (nullable = true)\n",
      " |-- NumSources: integer (nullable = true)\n",
      " |-- NumArticles: integer (nullable = true)\n",
      " |-- AvgTone: double (nullable = true)\n",
      " |-- Actor1Geo_Type: integer (nullable = true)\n",
      " |-- Actor1Geo_FullName: string (nullable = true)\n",
      " |-- Actor1Geo_CountryCode: string (nullable = true)\n",
      " |-- Actor1Geo_ADM1Code: string (nullable = true)\n",
      " |-- Actor1Geo_ADM2Code: string (nullable = true)\n",
      " |-- Actor1Geo_Lat: double (nullable = true)\n",
      " |-- Actor1Geo_Long: double (nullable = true)\n",
      " |-- Actor1Geo_FeatureID: string (nullable = true)\n",
      " |-- Actor2Geo_Type: integer (nullable = true)\n",
      " |-- Actor2Geo_FullName: string (nullable = true)\n",
      " |-- Actor2Geo_CountryCode: string (nullable = true)\n",
      " |-- Actor2Geo_ADM1Code: string (nullable = true)\n",
      " |-- Actor2Geo_ADM2Code: string (nullable = true)\n",
      " |-- Actor2Geo_Lat: double (nullable = true)\n",
      " |-- Actor2Geo_Long: double (nullable = true)\n",
      " |-- Actor2Geo_FeatureID: string (nullable = true)\n",
      " |-- ActionGeo_Type: integer (nullable = true)\n",
      " |-- ActionGeo_FullName: string (nullable = true)\n",
      " |-- ActionGeo_CountryCode: string (nullable = true)\n",
      " |-- ActionGeo_ADM1Code: string (nullable = true)\n",
      " |-- ActionGeo_ADM2Code: string (nullable = true)\n",
      " |-- ActionGeo_Lat: double (nullable = true)\n",
      " |-- ActionGeo_Long: double (nullable = true)\n",
      " |-- ActionGeo_FeatureID: string (nullable = true)\n",
      " |-- DATEADDED: long (nullable = true)\n",
      " |-- SOURCEURL: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 15:01:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+----------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|GLOBALEVENTID|   SQLDATE|MonthYear|Year|FractionDate|Actor1Code|Actor1Name|Actor1CountryCode|Actor1KnownGroupCode|Actor1EthnicCode|Actor1Religion1Code|Actor1Religion2Code|Actor1Type1Code|Actor1Type2Code|Actor1Type3Code|Actor2Code|Actor2Name|Actor2CountryCode|Actor2KnownGroupCode|Actor2EthnicCode|Actor2Religion1Code|Actor2Religion2Code|Actor2Type1Code|Actor2Type2Code|Actor2Type3Code|IsRootEvent|EventCode|EventBaseCode|EventRootCode|QuadClass|GoldsteinScale|NumMentions|NumSources|NumArticles|         AvgTone|Actor1Geo_Type|Actor1Geo_FullName|Actor1Geo_CountryCode|Actor1Geo_ADM1Code|Actor1Geo_ADM2Code|Actor1Geo_Lat|Actor1Geo_Long|Actor1Geo_FeatureID|Actor2Geo_Type|Actor2Geo_FullName|Actor2Geo_CountryCode|Actor2Geo_ADM1Code|Actor2Geo_ADM2Code|Actor2Geo_Lat|Actor2Geo_Long|Actor2Geo_FeatureID|ActionGeo_Type|ActionGeo_FullName|ActionGeo_CountryCode|ActionGeo_ADM1Code|ActionGeo_ADM2Code|ActionGeo_Lat|ActionGeo_Long|ActionGeo_FeatureID|     DATEADDED|           SOURCEURL|\n",
      "+-------------+----------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+----------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|    838788879|2018-04-16|   201804|2018|   2018.2904|      null|      null|             null|                null|            null|               null|               null|           null|           null|           null|       IRL|   IRELAND|              IRL|                null|            null|               null|               null|           null|           null|           null|          1|       43|           43|            4|        1|           2.8|         10|         1|         10|3.58208955223881|             0|              null|                 null|              null|              null|         null|          null|               null|             0|              null|                 null|              null|              null|         null|          null|               null|             0|              null|                 null|              null|              null|         null|          null|               null|20190416151500|https://www.eveni...|\n",
      "|    838788880|2018-04-16|   201804|2018|   2018.2904|       BUS|  INVESTOR|             null|                null|            null|               null|               null|            BUS|           null|           null|      null|      null|             null|                null|            null|               null|               null|           null|           null|           null|          1|      874|           87|            8|        2|          10.0|         10|         1|         10|2.48520710059172|             1|           Germany|                   GM|                GM|              null|         51.5|          10.5|                 GM|             0|              null|                 null|              null|              null|         null|          null|               null|             1|           Germany|                   GM|                GM|              null|         51.5|          10.5|                 GM|20190416151500|https://www.busin...|\n",
      "+-------------+----------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+----------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(events.columns) == len(headers['tableId'])\n",
    "for idx in range(len(events.columns)):\n",
    "    events = events.withColumnRenamed(f\"_c{idx}\", list(headers['tableId'])[idx])\n",
    "events = events.withColumn(\"SQLDATE\", to_date(col(\"SQLDATE\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "events.printSchema()\n",
    "events.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "598ad08b-2f7c-4b8b-9dd9-c7a0815ab33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events before cleaning: 1772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events after cleaning: 1108\n",
      "+-------------+----------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|GLOBALEVENTID|   SQLDATE|MonthYear|Year|FractionDate|Actor1Code|Actor1Name|Actor1CountryCode|Actor1KnownGroupCode|Actor1EthnicCode|Actor1Religion1Code|Actor1Religion2Code|Actor1Type1Code|Actor1Type2Code|Actor1Type3Code|Actor2Code|Actor2Name|Actor2CountryCode|Actor2KnownGroupCode|Actor2EthnicCode|Actor2Religion1Code|Actor2Religion2Code|Actor2Type1Code|Actor2Type2Code|Actor2Type3Code|IsRootEvent|EventCode|EventBaseCode|EventRootCode|QuadClass|GoldsteinScale|NumMentions|NumSources|NumArticles|          AvgTone|Actor1Geo_Type|  Actor1Geo_FullName|Actor1Geo_CountryCode|Actor1Geo_ADM1Code|Actor1Geo_ADM2Code|Actor1Geo_Lat|Actor1Geo_Long|Actor1Geo_FeatureID|Actor2Geo_Type|  Actor2Geo_FullName|Actor2Geo_CountryCode|Actor2Geo_ADM1Code|Actor2Geo_ADM2Code|Actor2Geo_Lat|Actor2Geo_Long|Actor2Geo_FeatureID|ActionGeo_Type|  ActionGeo_FullName|ActionGeo_CountryCode|ActionGeo_ADM1Code|ActionGeo_ADM2Code|ActionGeo_Lat|ActionGeo_Long|ActionGeo_FeatureID|     DATEADDED|           SOURCEURL|\n",
      "+-------------+----------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "|    838788881|2018-04-16|   201804|2018|   2018.2904|       EDU| ECONOMIST|             null|                null|            null|               null|               null|            EDU|           null|           null|       GOV| REGULATOR|             null|                null|            null|               null|               null|            GOV|           null|           null|          1|       20|           20|            2|        1|           3.0|         10|         1|         10|-3.15315315315315|             4|Vancouver, Britis...|                   CA|              CA02|             12552|        49.25|      -123.133|            -575268|             4|Vancouver, Britis...|                   CA|              CA02|             12552|        49.25|      -123.133|            -575268|             4|Vancouver, Britis...|                   CA|              CA02|             12552|        49.25|      -123.133|            -575268|20190416151500|https://www.bnnbl...|\n",
      "|    838788882|2018-04-16|   201804|2018|   2018.2904|       EDU|   STUDENT|             null|                null|            null|               null|               null|            EDU|           null|           null|       USA| LAS VEGAS|              USA|                null|            null|               null|               null|           null|           null|           null|          1|       36|           36|            3|        1|           4.0|          8|         1|          8|  2.2140221402214|             2|Illinois, United ...|                   US|              USIL|              null|      40.3363|      -89.0022|                 IL|             2|Illinois, United ...|                   US|              USIL|              null|      40.3363|      -89.0022|                 IL|             2|Illinois, United ...|                   US|              USIL|              null|      40.3363|      -89.0022|                 IL|20190416151500|https://today.iit...|\n",
      "+-------------+----------+---------+----+------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+----------+----------+-----------------+--------------------+----------------+-------------------+-------------------+---------------+---------------+---------------+-----------+---------+-------------+-------------+---------+--------------+-----------+----------+-----------+-----------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+---------------------+------------------+------------------+-------------+--------------+-------------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of events before cleaning: {events.count()}\")\n",
    "events_clean = events.filter('Actor1Code is not Null and Actor2Code is not Null and Actor1Geo_Lat is not Null and Actor1Geo_Long is not Null and Actor2Geo_Lat is not Null and Actor2Geo_Long is not Null')\n",
    "print(f\"Total number of events after cleaning: {events_clean.count()}\")\n",
    "events_clean.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966aca08-2ec1-4344-b916-042eb4d742c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cache_http_calls:\n",
    "    call_cache = call_cache = cachetools.TTLCache(10000, ttl=timedelta(seconds=1), timer=datetime.now)\n",
    "else:\n",
    "    call_cache = call_cache = cachetools.TTLCache(10000, ttl=timedelta(hours=1), timer=datetime.now)\n",
    "\n",
    "def get_model_response(payload: dict[str, object], event_id=None):\n",
    "    '''\n",
    "    Sample payload: \n",
    "    payload = {\n",
    "        \"data\": {\n",
    "            \"avg_tone\": -2,\n",
    "            \"goldstein\": 0.5,\n",
    "            \"actor_code\": \"GOV\",\n",
    "            \"lat\": 38,\n",
    "            \"lon\": -78,\n",
    "            \"date\": \"2018-10-23 04:30:00\"\n",
    "            }\n",
    "        }\n",
    "    TODO: Need to check if we need to implement politeness while calling the API\n",
    "    '''\n",
    "    if event_id and event_id in call_cache:\n",
    "        return call_cache['event_id']\n",
    "    \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    username = password = os.getenv('API_PASSWORD')\n",
    "\n",
    "    res = requests.post(\n",
    "        url=os.getenv('API_URL'),\n",
    "        headers = headers,\n",
    "        data = json.dumps(payload),\n",
    "        auth=(username, password)\n",
    "    )\n",
    "    response = json.loads(res.content.decode('utf-8'))\n",
    "    call_cache[event_id] = response\n",
    "    return response\n",
    "\n",
    "def flatten_model_response(actor: str, response: dict[str,object], event_id=None, debug=False):\n",
    "    d = {}\n",
    "    if event_id and debug: \n",
    "        print(event_id)\n",
    "    try:\n",
    "        d[f'{actor}_model_time_in_ms'] = response['model_time_in_ms']\n",
    "        d[f'{actor}_release_harness_version'] = response['release']['harness_version']\n",
    "        d[f'{actor}_release_model_version'] = response['release']['model_version']\n",
    "        d[f'{actor}_release_model_version_number'] = response['release']['model_version_number']\n",
    "        d[f'{actor}_request_id'] = response['request_id']\n",
    "        d[f'{actor}_result_class1'] = response['result']['class1']\n",
    "        d[f'{actor}_result_class2'] = response['result']['class2']\n",
    "        d[f'{actor}_timing'] = response['timing']\n",
    "    except Exception as e: \n",
    "        print(response)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bdccbb1-3158-4591-b397-39c9e7dcd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_payload(avg_tone, goldstein, actor_code, lat, lon, date):\n",
    "    data = {}\n",
    "    data['avg_tone'] = avg_tone\n",
    "    data['goldstein'] = goldstein\n",
    "    data['actor_code'] = actor_code\n",
    "    data['lat'] = lat\n",
    "    data['lon'] = lon\n",
    "    data['date'] = date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    payload = {}\n",
    "    payload['data'] = data\n",
    "    return payload\n",
    "    \n",
    "\n",
    "def call_model_output(row):\n",
    "    '''\n",
    "        payload = {\n",
    "        \"data\": {\n",
    "            \"avg_tone\": -2,\n",
    "            \"goldstein\": 0.5,\n",
    "            \"actor_code\": \"GOV\",\n",
    "            \"lat\": 38,\n",
    "            \"lon\": -78,\n",
    "            \"date\": \"2018-10-23 04:30:00\"\n",
    "            }\n",
    "        }\n",
    "    '''\n",
    "    # actor 1\n",
    "    r = row.asDict(True)\n",
    "    payload = create_payload(row['AvgTone'], row['GoldsteinScale'], row['Actor1Code'], row['Actor1Geo_Lat'], row['Actor1Geo_Long'], datetime.strptime(str(row['DATEADDED']),'%Y%m%d%H%M%S'))\n",
    "    response = flatten_model_response('Actor1', get_model_response(payload), event_id=row['GLOBALEVENTID'])\n",
    "\n",
    "    for k, v in response.items():\n",
    "        r[k] = v    \n",
    "    \n",
    "    # actor 2\n",
    "    payload = create_payload(row['AvgTone'], row['GoldsteinScale'], row['Actor2Code'], row['Actor2Geo_Lat'], row['Actor2Geo_Long'], datetime.strptime(str(row['DATEADDED']),'%Y%m%d%H%M%S'))\n",
    "    response = flatten_model_response('Actor2', get_model_response(payload), event_id=row['GLOBALEVENTID'])\n",
    "    \n",
    "    for k, v in response.items():\n",
    "        r[k] = v    \n",
    "    \n",
    "    return Row(**r)\n",
    "\n",
    "def define_schema(events):\n",
    "    schema = deepcopy(events.schema)\n",
    "    print('Number of columns in schema before addition = {:,}'.format(len(schema)))\n",
    "    # https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
    "    for actor in ['Actor1', 'Actor2']:\n",
    "        schema.add(StructField(f'{actor}__model_time_in_ms', IntegerType(), True))\n",
    "        schema.add(StructField(f'{actor}_release_harness_version', StringType(), True))\n",
    "        schema.add(StructField(f'{actor}_release_model_version', StringType(), True))\n",
    "        schema.add(StructField(f'{actor}_release_model_version_number', IntegerType(), True))\n",
    "        schema.add(StructField(f'{actor}_request_id', StringType(), True))\n",
    "        schema.add(StructField(f'{actor}_result_class1', BooleanType(), True))\n",
    "        schema.add(StructField(f'{actor}_result_class2', IntegerType(), True))\n",
    "        schema.add(StructField(f'{actor}_timing', DoubleType(), True))\n",
    "    print('Number of columns in schema after addition = {:,}'.format(len(schema)))\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1182e4-976c-416a-8a4e-ddd7b439a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in schema before addition = 61\n",
      "Number of columns in schema after addition = 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------\n",
      " GLOBALEVENTID                       | 838788881            \n",
      " SQLDATE                             | 2018-04-16           \n",
      " MonthYear                           | 201804               \n",
      " Year                                | 2018                 \n",
      " FractionDate                        | 2018.2904            \n",
      " Actor1Code                          | EDU                  \n",
      " Actor1Name                          | ECONOMIST            \n",
      " Actor1CountryCode                   | null                 \n",
      " Actor1KnownGroupCode                | null                 \n",
      " Actor1EthnicCode                    | null                 \n",
      " Actor1Religion1Code                 | null                 \n",
      " Actor1Religion2Code                 | null                 \n",
      " Actor1Type1Code                     | EDU                  \n",
      " Actor1Type2Code                     | null                 \n",
      " Actor1Type3Code                     | null                 \n",
      " Actor2Code                          | GOV                  \n",
      " Actor2Name                          | REGULATOR            \n",
      " Actor2CountryCode                   | null                 \n",
      " Actor2KnownGroupCode                | null                 \n",
      " Actor2EthnicCode                    | null                 \n",
      " Actor2Religion1Code                 | null                 \n",
      " Actor2Religion2Code                 | null                 \n",
      " Actor2Type1Code                     | GOV                  \n",
      " Actor2Type2Code                     | null                 \n",
      " Actor2Type3Code                     | null                 \n",
      " IsRootEvent                         | 1                    \n",
      " EventCode                           | 20                   \n",
      " EventBaseCode                       | 20                   \n",
      " EventRootCode                       | 2                    \n",
      " QuadClass                           | 1                    \n",
      " GoldsteinScale                      | 3.0                  \n",
      " NumMentions                         | 10                   \n",
      " NumSources                          | 1                    \n",
      " NumArticles                         | 10                   \n",
      " AvgTone                             | -3.15315315315315    \n",
      " Actor1Geo_Type                      | 4                    \n",
      " Actor1Geo_FullName                  | Vancouver, Britis... \n",
      " Actor1Geo_CountryCode               | CA                   \n",
      " Actor1Geo_ADM1Code                  | CA02                 \n",
      " Actor1Geo_ADM2Code                  | 12552                \n",
      " Actor1Geo_Lat                       | 49.25                \n",
      " Actor1Geo_Long                      | -123.133             \n",
      " Actor1Geo_FeatureID                 | -575268              \n",
      " Actor2Geo_Type                      | 4                    \n",
      " Actor2Geo_FullName                  | Vancouver, Britis... \n",
      " Actor2Geo_CountryCode               | CA                   \n",
      " Actor2Geo_ADM1Code                  | CA02                 \n",
      " Actor2Geo_ADM2Code                  | 12552                \n",
      " Actor2Geo_Lat                       | 49.25                \n",
      " Actor2Geo_Long                      | -123.133             \n",
      " Actor2Geo_FeatureID                 | -575268              \n",
      " ActionGeo_Type                      | 4                    \n",
      " ActionGeo_FullName                  | Vancouver, Britis... \n",
      " ActionGeo_CountryCode               | CA                   \n",
      " ActionGeo_ADM1Code                  | CA02                 \n",
      " ActionGeo_ADM2Code                  | 12552                \n",
      " ActionGeo_Lat                       | 49.25                \n",
      " ActionGeo_Long                      | -123.133             \n",
      " ActionGeo_FeatureID                 | -575268              \n",
      " DATEADDED                           | 20190416151500       \n",
      " SOURCEURL                           | https://www.bnnbl... \n",
      " Actor1__model_time_in_ms            | 1000                 \n",
      " Actor1_release_harness_version      | 0.1                  \n",
      " Actor1_release_model_version        | 5ec427ae4cedfd000... \n",
      " Actor1_release_model_version_number | 4                    \n",
      " Actor1_request_id                   | KTF1CH0MAY9Q74SR     \n",
      " Actor1_result_class1                | true                 \n",
      " Actor1_result_class2                | 3                    \n",
      " Actor1_timing                       | 1000.3688335418701   \n",
      " Actor2__model_time_in_ms            | 0                    \n",
      " Actor2_release_harness_version      | 0.1                  \n",
      " Actor2_release_model_version        | 5ec427ae4cedfd000... \n",
      " Actor2_release_model_version_number | 4                    \n",
      " Actor2_request_id                   | RGW2IH58DNBT4AFS     \n",
      " Actor2_result_class1                | true                 \n",
      " Actor2_result_class2                | 3                    \n",
      " Actor2_timing                       | 0.07510185241699219  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/jovyan/work/model_output.parquet already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2919/2354079784.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_output.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/jovyan/work/model_output.parquet already exists."
     ]
    }
   ],
   "source": [
    "df = events_clean.rdd.map(call_model_output)\n",
    "schema = define_schema(events_clean)\n",
    "df = spark.createDataFrame(df, schema)\n",
    "df.show(n=1, vertical=True)\n",
    "df.write.parquet('model_output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8deae8-5470-46c1-a7f5-08116ce41039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('model_output.parquet')\n",
    "dfp = df.toPandas()\n",
    "dfp.to_csv('model_output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
